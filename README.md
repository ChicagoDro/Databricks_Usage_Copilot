# Who Am I?
**Pete Tamisin** â€“ Technical GTM Leader â€¢ AI & Data Engineering Architect â€¢ Builder & Teacher
Based in Chicago, IL.

* 20+ years designing data & AI platforms (Dir. at Capital One, ex-Databricks, 2x series A startup exits, x-Siemens, x-Motorola)
* Focused on **modern data platforms**, **Context aware RAG systems**, and **enterprise GenAI adoption**
* Passionate about **teaching** and helping teams ship real-world AI systems

ğŸ“§ Email: `pete@tamisin.com`
ğŸ”— LinkedIn: [peter-tamisin-50a3233a](https://www.linkedin.com/in/peter-tamisin-50a3233a/)

---

# Databricks Usage Copilot

A fully-featured **enterprise Retrieval-Augmented Generation (RAG)** system that turns Databricks-style platform usage telemetry into an interactive AI assistant.

This project ingests structured Databricks-like operational data into:

- A **SQLite database** with realistic usage tables  
- A **FAISS vector index** for semantic retrieval  
- An **in-memory graph** of your environment (org units â†’ users â†’ jobs â†’ runs â†’ usage â†’ events â†’ evictions â†’ SQL queries)  
- A **Graph-aware orchestrator** that performs graph expansion + semantic retrieval  
- A **Streamlit UI** + **CLI** that show both the answer *and* â€œhow the AI reasonedâ€

The result is an **AI copilot** capable of answering questions such as:

- â€œWhy did this job cost so much yesterday?â€  
- â€œShow me SQL queries contributing most to Finance warehouse spend.â€  
- â€œWhat spot evictions have impacted ML workloads recently?â€  
- â€œWhich org unit owns the compute driving last weekâ€™s DBU spike?â€  
- â€œWhich jobs need optimizing based on total cost?â€  

This repo demonstrates a **production-style architecture** for enterprise LLM applications built on **RAG + graphs + orchestration**.

---

## ğŸ“¦ Dataset Overview

All data is stored in a local **SQLite database**:

```text
data/Databricks_Usage/usage_rag_data.db
````

This database is generated from:

* `data/Databricks_Usage/create_usage_tables.sql` â€” table definitions
* `data/Databricks_Usage/seed_usage_tables.sql` â€” synthetic but realistic seed data
* `database_setup.py` â€” orchestration script that creates & seeds the database

### Tables & Concepts

| Table               | Description                                                  |
| ------------------- | ------------------------------------------------------------ |
| `company_ou`        | Organizational units (cost centers)                          |
| `users_lookup`      | Users, departments, and OU membership                        |
| `jobs`              | Scheduled Databricks jobs with metadata + tags               |
| `job_runs`          | Daily executions of jobs with status + cluster settings      |
| `compute_usage`     | DBU usage, cost, instance type, CPU/memory metrics           |
| `non_job_compute`   | SQL Warehouses & All-Purpose Clusters                        |
| `events`            | Cluster lifecycle events and spot eviction-related events    |
| `eviction_details`  | Detailed spot eviction telemetry                             |
| `sql_query_history` | Ad-hoc SQL query executions (user, warehouse, duration, SQL) |
| `date_series`       | Synthetic daily range used to generate runs/usage            |

### Dataset Purpose

The dataset is intentionally **relational** and **interconnected** to mimic real telemetry:

* **Jobs** â†’ **job runs** â†’ **compute usage**
* **Usage** â†’ **events** â†’ **evictions**
* **Users** â†’ **queries** and **org units**

This makes it ideal for demonstrating:

* **Graphs & relationships** (jobs â†’ runs â†’ usage â†’ events â†’ queries)
* **Context assembly via traversal**
* **Hybrid retrieval** (semantic + structural)
* **RAG systems for FinOps / observability / governance**

---

## ğŸ•¸ï¸ Architecture Overview (GraphRAG + UI)

At a high level:

1. **SQLite DB** holds structured Databricks usage data.
2. `ingest_usage_domain.py` reads the DB and turns rows into **RAG documents**.
3. `ingest_embed_index.py` embeds those docs and stores them in a **FAISS index**.
4. `graph_model.py` builds **nodes and edges** representing org structure and workload relationships.
5. `graph_retriever.py`:

   * Uses vector search to find **anchor documents** for a query
   * Expands a **subgraph** around those anchors using BFS
   * Collects all relevant docs for context
6. `chat_orchestrator.py`:

   * Classifies the **question type** (global aggregate, global top-N, local explanation, etc.)
   * Routes to **deterministic graph logic** when appropriate (e.g., â€œhow many jobs?â€, â€œwhich jobs need optimizing?â€)
   * Otherwise calls the **GraphRAG retriever + LLM**
   * Returns an answer, a **graph explanation**, and the **LLM prompt + context** used
7. `app.py` (Streamlit UI):

   * Renders a chat interface
   * Shows an expandable **â€œHow I reasonedâ€** panel with:

     * Graph subgraph summary
     * The prompt sent to the LLM
     * The context passed into the prompt

### Diagram

```text
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   SQLite Usage DB      â”‚
                  â”‚ (jobs, runs, usage,    â”‚
                  â”‚  events, queries, OU)  â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚ SQL (SELECT)
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚ ingest_usage_domain.py  â”‚
                 â”‚  - Rows â†’ RAG docs      â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚ Documents
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚ ingest_embed_index.py   â”‚
                 â”‚  - Embeddings           â”‚
                 â”‚  - FAISS index          â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚ Vector search
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   graph_model.py          â”‚
                â”‚  - Nodes & edges          â”‚
                â”‚  - Adjacency (BFS)        â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚ node_ids
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   graph_retriever.py      â”‚
                â”‚  - Vector anchors (FAISS) â”‚
                â”‚  - Graph expansion (BFS)  â”‚
                â”‚  - Context assembly       â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚ context docs
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   chat_orchestrator.py     â”‚
                â”‚  - Question classifier     â”‚
                â”‚  - Global aggregates      â”‚
                â”‚  - GraphRAG + LLM          â”‚
                â”‚  - Debug prompt + context  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚ answer + debug
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Streamlit UI (app.py)    â”‚
                â”‚  - Chat                    â”‚
                â”‚  - "How I reasoned" panel  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§­ Graph vs. Vector Routing

The assistant doesnâ€™t treat every question the same.  
Some questions are best answered by **direct graph aggregation**, others by **GraphRAG** (vector + graph), and a few fall back to **plain vector RAG**.

At a high level:

- **Vector search** answers:  
  > â€œWhat are we talking about?â€  
  (Find the most relevant nodes/docs.)

- **Graph traversal / aggregation** answers:  
  > â€œWhat else is related?â€ and â€œHow do we compute totals, rankings, or coverage across the whole environment?â€

### ğŸ”€ Routing Strategies

The `DatabricksUsageAssistant` routes questions through three main paths:

1. **Global Graph Aggregates (Graph-only, no LLM reasoning needed)**
   - Examples:
     - â€œHow many jobs are there?â€
     - â€œHow many users do we have?â€
   - Behavior:
     - Skip vector search
     - Directly inspect the graph (count nodes by type)
     - Return a deterministic answer like:
       > â€œThere are 5 jobs in this environmentâ€¦â€

2. **Global Usage & Top-N (Graph Aggregation + LLM Copyediting)**
   - Examples:
     - â€œTell me about my Databricks usage.â€
     - â€œGive me a summary of my job usage.â€
     - â€œWhich jobs need optimizing?â€
     - â€œTop 3 most expensive jobs.â€
   - Behavior:
     - Skip GraphRAG neighborhood
     - Traverse the graph:
       - `compute_usage` â†’ `job_run` â†’ `job`
     - Aggregate `cost_usd` per job
     - Rank, compute shares of total spend, etc.
     - Use the LLM to turn those numbers into a readable explanation.

3. **GraphRAG (Vector + Graph Expansion + LLM)**
   - Examples:
     - â€œWhy is the HR Dashboard Prep job expensive?â€
     - â€œWhat happened around the last eviction in Logistics?â€
   - Behavior:
     - Use **vector search** over FAISS to find **anchor docs**
       (e.g., job J-HR-DASH, its runs, a usage record).
     - Expand a **subgraph** around those anchors with BFS:
       - job â†’ runs â†’ usage â†’ events â†’ evictions â†’ user
     - Render that neighborhood into a context string.
     - Feed context + question into the LLM.
     - Return the answer and a â€œHow I reasonedâ€ explanation.

If the classifier canâ€™t confidently categorize the question, the system defaults to the **GraphRAG** path.

---

### ğŸ§  Routing Flow Diagram

```text
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚           User Question                â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Classifier + Heuristicsâ”‚
                    â”‚  (intent, entity_type)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                     â”‚                     â”‚
          â–¼                     â–¼                     â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Global Aggregateâ”‚   â”‚ Global Usage / Top-N â”‚  â”‚   Local / Other      â”‚
 â”‚ (counts)        â”‚   â”‚ (cost, ranking)      â”‚  â”‚ (explanations, why?) â”‚
 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                        â”‚                         â”‚
        â–¼                        â–¼                         â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Graph-only    â”‚       â”‚ Graph-only     â”‚      â”‚  GraphRAG              â”‚
 â”‚ (node counts) â”‚       â”‚ aggregates     â”‚      â”‚  1) Vector anchors     â”‚
 â”‚               â”‚       â”‚ (cost by job,  â”‚      â”‚  2) BFS subgraph       â”‚
 â”‚ e.g. jobs,    â”‚       â”‚  top-N jobs)   â”‚      â”‚  3) Context + LLM      â”‚
 â”‚ users, OUs    â”‚       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚                           â”‚
        â”‚                        â”‚                           â”‚
        â–¼                        â–¼                           â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Deterministic   â”‚   â”‚ Deterministic + LLM   â”‚   â”‚ LLM Answer          â”‚
 â”‚ answer string   â”‚   â”‚ narrative (optional)  â”‚   â”‚ (with graph context)â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                        â”‚                         â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–¼                       â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚  Answer           â”‚   â”‚ "How I reasoned" panel  â”‚
             â”‚                   â”‚   â”‚ - Subgraph summary      â”‚
             â”‚                   â”‚   â”‚ - Prompt + context      â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
````

This routing lets the assistant:

* Use **graphs** where structure matters (counts, cost aggregation, relationships).
* Use **vector search + graph expansion** where semantics matter (â€œwhy did this job behave this way?â€).
* Stay **transparent**, thanks to the â€œHow I reasonedâ€ panel exposing the graph, prompt, and context.

```
```

---

## ğŸ§  Why This Architecture?

### 1. Enterprise telemetry is a graph

Databricks usage is naturally modeled as:

* Org units â†’ users
* Users â†’ jobs & queries
* Jobs â†’ job runs â†’ compute usage
* Usage â†’ events â†’ evictions

Answering **â€œwhyâ€**, **â€œwhoâ€**, and **â€œwhat else is relatedâ€** requires following **relationships**, not just matching text.

### 2. Pure vector RAG struggles on structural questions

Example:

> â€œWhy did job J-LOGI-OPT fail yesterday?â€

Pure vector search might miss:

* The specific **job run** that failed
* The **eviction event** tied to that run
* The **compute usage** record that shows spot capacity
* The **queries** that ran shortly before

GraphRAG ensures those nodes are traversed and included in context.

### 3. Hybrid = semantic + structural power

* **Vector search** â†’ finds *what the user is talking about*
* **Graph traversal** â†’ finds *everything structurally related*
* **LLM** â†’ synthesizes an answer with full context

This is the pattern youâ€™d want for a real **FinOps / observability / governance copilot**.

### 4. Transparent reasoning (â€œHow I reasonedâ€)

Each answer includes:

* A **graph explanation**: what node types were used, how many, and some example nodes
* The **exact prompt** sent to the LLM (system prompt + context + user question)
* The **context** (rendered docs from the graph neighborhood)

This is perfect for:

* Debugging â€œwhy did it only talk about one job?â€
* Showing platform teams how the AI arrived at its answer
* Teaching others how GraphRAG flows work

---

## ğŸ§© Design Challenges & How We Solved Them

This project isnâ€™t just a happy path â€” it documents some **real graph/RAG issues** and how we fixed them.

### 1. Parentâ€“Child Edge Direction

**Problem:**
Initially, graph edges were modeled only **child â†’ parent**:

* `job_run` â†’ `job` (`RUN_OF`)
* `compute_usage` â†’ `job_run` (`USAGE_OF_JOB_RUN`)
* `event` â†’ `usage` (`ON_USAGE`)

This is natural from a â€œthis thing belongs to thatâ€ perspective, but made it hard to answer questions like:

> â€œSummarize usage for each job.â€

Because from the **job** nodeâ€™s perspective, there were no outgoing edges to its runs/usages.

**Fix: Reverse â€œHAS_*â€ edges**

In `graph_model.py` we kept the original edges but added **reverse parent â†’ child** edges:

* `job` â†’ `job_run` (`HAS_RUN`)
* `job_run` â†’ `compute_usage` (`HAS_USAGE`)
* `usage` â†’ `event` (`HAS_EVENT`)
* `user` â†’ `query` (`HAS_QUERY`)

Now we can easily traverse **from a job** down to all of its runs â†’ usage â†’ events without doing expensive reverse lookups.

> **Lesson:** For GraphRAG, itâ€™s often worth maintaining **both directions** (semantic: â€œRUN_OFâ€, ergonomic: â€œHAS_RUNâ€).

---

### 2. â€œWhy am I only seeing one job?â€ (Routing & Coverage)

**Problem:**
For global-sounding questions like:

> â€œgive me a summary of my job usageâ€

the retriever was:

1. Doing a semantic search on that text.
2. Picking a single **anchor job** (e.g., `J-HR-DASH`) and its neighborhood.
3. Building context entirely around that one job.

So the LLM answer looked reasonable, but it only talked about **one job**, not **all five**.

**Fix: Question-type routing in `chat_orchestrator.py`**

We introduced a **classifier + heuristics** that route certain question types away from the pure GraphRAG path and into **deterministic graph-based logic**:

* **Global aggregates**

  * Intent: `"global_aggregate"`
  * Examples:

    * â€œHow many jobs are there?â€
    * â€œHow many users do we have?â€
  * Solution: `_answer_global_aggregate` â†’ counts nodes by type directly from the graph.

* **Global top-N (jobs)**

  * Intent: `"global_topn"` + `entity_type=="job"`
  * Example:

    * â€œTop 3 most expensive jobsâ€
  * Solution: `_answer_global_topn_jobs` â†’ aggregates `compute_usage.cost_usd` per job and ranks.

* **Global usage overview**

  * Heuristic: `_looks_like_usage_overview_question`
  * Examples:

    * â€œtell me about my databricks usageâ€
    * â€œsummary of my job usageâ€
  * Solution: `_answer_global_usage_overview` â†’ aggregates cost for **all jobs** and returns a full breakdown.

* **Jobs needing optimization**

  * Heuristic: `_looks_like_jobs_optimization_question`
  * Example:

    * â€œwhich jobs need optimizing?â€
  * Solution: `_answer_jobs_needing_optimization` â†’ surfaces the highest-cost jobs by share of total spend.

If a question matches one of these, it **never** goes down the â€œsingle anchor GraphRAGâ€ path â€” it uses **all jobs** via graph aggregation.

> **Lesson:** Not every question should be answered via â€œretrieve a neighborhood + LLM.â€
> Some are better served by **explicit graph computations**.

---

### 3. Debugging Context & Prompt (â€œHow I reasonedâ€ Panel)

**Problem:**
When debugging, it wasnâ€™t clear:

* Which nodes were actually included in the subgraph
* Which docs were sent as context
* What prompt the LLM actually saw

This made it difficult to answer:
â€œIs this a retrieval issue, a graph issue, or a language-model issue?â€

**Fix: Rich `ChatResult` + Streamlit UI**

`ChatResult` now includes:

* `answer`: final LLM (or deterministic) answer
* `context_docs`: the retrieved `Document` objects
* `graph_explanation`: summary of node counts and sample nodes
* `llm_prompt`: the assembled prompt text (system + context + question)
* `llm_context`: the rendered context string

In `app.py`, the Streamlit UI adds an expander:

> ğŸ” **How I reasoned (GraphRAG explanation)**

Inside it you see:

* A human-readable description of the subgraph
* The **LLM prompt** (copy-pasteable for inspection)
* The **context** passed to the LLM

This made it immediately obvious when a â€œglobalâ€ question was only seeing one job in the context â€” which pointed straight back to routing and retriever behavior instead of the DB or graph.

> **Lesson:** For serious RAG/GraphRAG, invest in **debug visibility**.
> Being able to see prompt + context + graph summary is huge.

---

## âš™ï¸ Setup & Installation

> These instructions assume a working Python 3.10+ and `git`.

### 1. Clone the repository

```bash
git clone https://github.com/ChicagoDro/AI-Portfolio
cd AI-Portfolio
```

### 2. Create & activate a virtual environment

**macOS / Linux:**

```bash
python3 -m venv .venv
source .venv/bin/activate
```

**Windows (PowerShell):**

```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
```

### 3. Install dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### 4. Configure environment variables

Create a `.env` file in the project root with at least:

```env
LLM_PROVIDER=openai          # or gemini, etc.
OPENAI_API_KEY=sk-...        # if using OpenAI

# Optional: override models
# OPENAI_CHAT_MODEL=gpt-4.1-mini
# OPENAI_EMBED_MODEL=text-embedding-3-small
```

---

# ğŸš¦ How to Run the System (Using the Makefile)

This project now includes a convenient **Makefile** to automate the entire workflow:

* Creating the SQLite database
* Building the FAISS vector index
* Launching the Streamlit UI
* Cleaning generated artifacts

You no longer need to manually run `database_setup.py`, `ingest_embed_index.py`, or `streamlit run â€¦`.
Just use `make`.

---

## ğŸ§° Available Make Targets

### **`make db` â€” Create & Seed the SQLite Database**

This target:

1. Runs `database_setup.py`
2. Creates `data/usage_rag_data.db`
3. Executes:

   * `create_usage_tables.sql`
   * `seed_usage_tables.sql`

This gives you a **fully populated Databricks-like usage database**, including:

* Jobs
* Job runs
* Compute usage
* Events
* Evictions
* SQL query history
* Users & org units

---

### **`make index` â€” Build the FAISS Semantic Index**

This target:

1. Runs the full domain ingestion pipeline (`src/ingest_usage_domain.py`)
2. Embeds every usage document
3. Builds a FAISS vector index
4. Saves it to:

```
indexes/usage_faiss/
```

The index is what allows the assistant to:

* Perform semantic retrieval
* Pick anchor nodes
* Trigger graph expansion for GraphRAG

---

### **`make app` â€” Launch the Streamlit UI**

This target:

1. Sets the correct `PYTHONPATH`
2. Boots the Streamlit interface at:

```
http://localhost:8501
```

The UI includes:

* Chat window
* Graph-aware explanations ("How I Reasoned")
* Debug info (prompt + context, if enabled)

This is the interactive Databricks Usage Copilot experience.

---

### **`make all` â€” Full Pipeline: DB â†’ Index â†’ UI**

This is the smoothest end-to-end experience.

Running:

```
make all
```

will:

1. Build / rebuild the SQLite database
2. Build / rebuild the FAISS index
3. Launch the Streamlit app immediately

Perfect for first-time setup or after making schema changes.

---

### **`make clean` â€” Remove All Generated Artifacts**

This target deletes:

* The SQLite database
* The FAISS index directory

Useful when:

* You want to regenerate everything from scratch
* You updated the schema or seed data
* Youâ€™re debugging ingestion or graph-building issues

---

## ğŸ¯ Recommended Workflow

To set up the system for the first time:

```bash
make all
```

After that, when you update:

* Seed data â†’ run `make db index`
* Embedding model or ingestion logic â†’ run `make index`
* UI only â†’ run `make app`
* Reset everything â†’ run `make clean && make all`

---

## ğŸ“Œ Under the Hood (What Each Step Actually Does)

| Make Target  | What Happens Internally                                                                 |
| ------------ | --------------------------------------------------------------------------------------- |
| `make db`    | Executes Python schema builder â†’ creates all tables â†’ inserts all synthetic records     |
| `make index` | Generates RAG docs â†’ computes embeddings â†’ builds FAISS index â†’ stores metadata         |
| `make app`   | Loads the FAISS index + graph â†’ initializes routing logic â†’ launches Streamlit frontend |
| `make clean` | Removes SQLite DB + FAISS index folder                                                  |
| `make all`   | `db` + `index` + `app`                                                                  |

---

## ğŸ§ª Tip: You Can Combine Targets

Make supports chaining:

```bash
make db index
```

or:

```bash
make index app
```

or even:

```bash
make db clean   # (not recommendedâ€”it deletes the DB right after!)
```

---

## ğŸ’¬ Example Prompts to Try

### Cost / FinOps

* â€œHow many jobs are there?â€
* â€œTop 3 most expensive jobs.â€
* â€œWhich jobs need optimizing based on cost?â€
* â€œBreak down DBU consumption by org unit.â€

### Reliability

* â€œWhat spot evictions have impacted ML jobs?â€
* â€œWhich runs of the Finance ETL job failed and why?â€

### Usage Overview

* â€œTell me about my Databricks usage.â€
* â€œGive me a summary of my job usage.â€

### Governance / Ownership

* â€œWhich org unit owns the compute driving last weekâ€™s DBU spike?â€
* â€œWhich users issued long-running queries yesterday?â€

---

## ğŸ“ Project Structure

```text
AI-Portfolio/
  database_setup.py               # SQLite setup + schema population
  .env                            # environment variables (ignored in git)
  requirements.txt                # dependencies

  data/
    create_usage_tables.sql     # Creates Databricks Usage schema
    seed_usage_tables.sql       # Loads sample data
    usage_rag_data.db           # Generated SQLite DB

  indexes/
    usage_faiss/                  # FAISS index (created at runtime)

  src/
    config.py                     # Paths + provider config
    ingest_usage_domain.py        # SQL â†’ RAG docs
    ingest_embed_index.py         # Docs â†’ embeddings â†’ FAISS
    graph_model.py                # Nodes & edges & adjacency (HAS_* edges)
    graph_retriever.py            # Graph-aware retriever (GraphRAG)
    chat_orchestrator.py          # LLM orchestration + routing + debug
    app.py                        # Streamlit UI ("How I reasoned" panel)
```

---

## ğŸš€ Portfolio Value Statement

You can honestly say:

> **â€œI designed and implemented a GraphRAG system that models Databricks usage telemetry as both a FAISS vector index and a graph of jobs, runs, compute usage, events, evictions, and SQL queries. I debugged real-world issues like edge direction (childâ†’parent vs parentâ†’child) and global vs local question routing, adding reverse â€˜HAS_*â€™ edges, explicit global aggregation paths, and a â€˜How I reasonedâ€™ panel that surfaces the exact prompt and context sent to the LLM. The result is an explainable FinOps / observability copilot that combines vector search, graph traversal, and LLM reasoning.â€**

---

## ğŸ”­ Future Enhancements

* Add a **router** that sends Databricks Best Practice PDFs to hybrid RAG and usage questions to GraphRAG.
* Integrate **LangGraph** for multi-step workflows and tools.
* Add an **evaluation harness** (groundedness, answer quality, coverage).
* Export the graph into **Neo4j** for large-scale graph analytics.
* Add **graph visualization** in the UI (e.g., job â†’ runs â†’ usage â†’ events view).

```
```
